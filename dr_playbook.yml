# dr_rh_certified.yml
# ────────────────────────────────────────────────────────────────
# Disaster-Recovery workflow that uses ONLY Red Hat-supported content
# ────────────────────────────────────────────────────────────────

###############################################################################
# PLAY 1 – PROTECT  (primary site)  ➜  create database + file backups
###############################################################################
- name: Stage 1 – Protect (Primary site)
  hosts: primary
  become: true # Connects as ansibleuser, then uses sudo for root privileges on primary-server
  vars:
    # Removed current_timestamp variable here.
    # Filenames will now be generated dynamically on the remote host.
    db_name: myapp
    db_user: postgres
    db_password: "pass@123"
    backup_root: /var/backups/dr
    app_data_path: /var/www/myapp
    control_node_fetch_dir: "/tmp/ansible_dr_fetched_backups"

  tasks:
    - name: Ensure backup directory exists on primary
      ansible.builtin.file:
        path: "{{ backup_root }}"
        state: directory
        mode: "0750"

    - name: Dump and compress PostgreSQL database
      # Use $(date +%Y%m%d%H%M%S) directly in the shell command for the remote host's timestamp
      ansible.builtin.shell: >
        pg_dump -U {{ db_user }} {{ db_name }} | gzip -9 -c > {{ backup_root }}/{{ db_name }}_$(date +%Y%m%d%H%M%S).sql.gz
      environment:
        PGPASSWORD: "{{ db_password }}"
      changed_when: true
      tags: [backup, database]

    - name: Create tarball of application data
      # Use $(date +%Y%m%d%H%M%S) directly in the shell command for the remote host's timestamp
      ansible.builtin.shell: >
        tar --create --gzip --file {{ backup_root }}/data_$(date +%Y%m%d%H%M%S).tgz -C {{ app_data_path }} .
      changed_when: true
      tags: [backup, files]

    - name: Wait 15 seconds for artifacts to settle on disk
      ansible.builtin.pause:
        seconds: 15

    - name: Find the latest created DB dump file
      ansible.builtin.find:
        paths: "{{ backup_root }}"
        patterns: "{{ db_name }}_*.sql.gz"
        # Sort by modification time to get the most recent one.
        # find's 'sortby: mtime' is ascending by default, so the newest is 'last'.
        file_type: file
      register: latest_db_dump

    - name: Find the latest created data archive file
      ansible.builtin.find:
        paths: "{{ backup_root }}"
        patterns: "data_*.tgz"
        sortby: mtime
        file_type: file
      register: latest_data_archive

    - name: DEBUG - Show discovered artifact paths
      ansible.builtin.debug:
        msg:
          - "Discovered DB Dump Path: {{ (latest_db_dump.files | last).path | default('Not found') }}"
          - "Discovered Data Archive Path: {{ (latest_data_archive.files | last).path | default('Not found') }}"

    - name: Fail if latest DB dump was not found
      ansible.builtin.assert:
        that:
          - latest_db_dump.matched > 0
          - (latest_db_dump.files | last).path is defined # Ensure path exists in the discovered file object
        fail_msg: "No recent DB dump found matching pattern {{ db_name }}_*.sql.gz in {{ backup_root }}"

    - name: Fail if latest data archive was not found
      ansible.builtin.assert:
        that:
          - latest_data_archive.matched > 0
          - (latest_data_archive.files | last).path is defined # Ensure path exists in the discovered file object
        fail_msg: "No recent data archive found matching pattern data_*.tgz in {{ backup_root }}"

    - name: Register artifacts for fetch (primary paths)
      ansible.builtin.set_fact:
        primary_backup_package:
          - "{{ (latest_db_dump.files | last).path }}"
          - "{{ (latest_data_archive.files | last).path }}"
      tags: always

    - name: Fetch backup artifacts from primary to control node
      ansible.builtin.fetch:
        src: "{{ item }}"
        dest: "{{ control_node_fetch_dir }}"
        flat: true
      loop: "{{ primary_backup_package }}"
      register: fetched_results

    - name: Register fetched artifact paths on control node
      ansible.builtin.set_fact:
        control_node_backup_package: |
          {% set fetched_paths = [] %}
          {% for result in fetched_results.results %}
            {% if result.dest is defined %}
              {% set _ = fetched_paths.append(result.dest) %}
            {% endif %}
          {% endfor %}
          {{ fetched_paths }}
      tags: always

###############################################################################
# PLAY 2 – TRANSFER  (Control Node ➜ DR host)  ➜  copy artifacts off-site
###############################################################################
- name: Stage 2 – Transfer (Control Node ➜ DR)
  hosts: dr
  become: true
  vars:
    dr_incoming: /srv/dr_incoming

  tasks:
    - name: Ensure incoming directory exists on DR host
      ansible.builtin.file:
        path: "{{ dr_incoming }}"
        state: directory
        mode: "0750"

    - name: Copy backup artifacts from control node to DR host
      ansible.builtin.copy:
        src: "{{ item }}"
        dest: "{{ dr_incoming }}/"
        mode: preserve
      loop: "{{ hostvars[groups['primary'][0]].control_node_backup_package }}"
      tags: [transfer]

###############################################################################
# PLAY 3 – RECOVER  (DR site)  ➜  restore when dr_action=failover
###############################################################################
- name: Stage 3 – Recover (only when dr_action=failover)
  hosts: dr
  become: true
  vars:
    dr_action: "{{ dr_action | default('standby') }}"
    incoming_dir: /srv/dr_incoming
    restore_root: /srv/dr_restore
    db_name: myapp
    db_user: postgres
    db_password: "{{ lookup('ansible.builtin.var', 'db_password', default='') }}"

  tasks:
    - name: Skip unless fail-over requested
      ansible.builtin.meta: end_play
      when: dr_action != 'failover'

    - name: Create restore directory
      ansible.builtin.file:
        path: "{{ restore_root }}"
        state: directory
        mode: "0750"

    - name: Find latest data archive
      ansible.builtin.find:
        paths: "{{ incoming_dir }}"
        patterns: "data_*.tgz"
        sortby: mtime
        file_type: file
      register: data_archives

    - name: Extract latest data archive
      ansible.builtin.unarchive:
        src: "{{ (data_archives.files | last).path }}" # Use | last for newest
        dest: "{{ restore_root }}"
        remote_src: true
      when: data_archives.matched > 0
      tags: [restore, files]

    - name: Find latest DB dump
      ansible.builtin.find:
        paths: "{{ incoming_dir }}"
        patterns: "{{ db_name }}_*.sql.gz"
        sortby: mtime
        file_type: file
      register: db_dumps

    - name: Restore PostgreSQL dump
      ansible.builtin.shell: >
        bash -c 'gunzip -c "{{ (db_dumps.files | last).path }}" |
                  psql -U {{ db_user }} {{ db_name }}'
      environment:
        PGPASSWORD: "{{ db_password }}"
      when: db_dumps.matched > 0
      changed_when: true
      tags: [restore, database]

    - name: Start application service
      ansible.builtin.systemd:
        name: myapp
        state: started
        enabled: true
      tags: [restore, services]

    - name: Check application health endpoint
      ansible.builtin.uri:
        url: http://localhost/health
        status_code: 200
        retries: 5
        delay: 10
        register: health_check
        until: health_check.status == 200
      tags: [validate]
