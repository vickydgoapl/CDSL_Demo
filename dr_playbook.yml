# dr_rh_certified.yml
# ────────────────────────────────────────────────────────────────
# Disaster-Recovery workflow that uses ONLY Red Hat-supported content
# ────────────────────────────────────────────────────────────────

###############################################################################
# PLAY 1 – PROTECT  (primary site)  ➜  create database + file backups
###############################################################################
- name: Stage 1 – Protect (Primary site)
  hosts: primary
  become: true # Connects as ansibleuser, then uses sudo for root privileges on primary-server
  vars:
    current_timestamp: "{{ '%Y%m%d%H%M%S' | strftime }}"
    db_name: myapp
    db_user: postgres
    db_password: "pass@123"
    dump_file: "{{ backup_root }}/{{ db_name }}_{{ current_timestamp }}.sql.gz"
    data_archive: "{{ backup_root }}/data_{{ current_timestamp }}.tgz"
    backup_root: /var/backups/dr
    app_data_path: /var/www/myapp
    control_node_fetch_dir: "/tmp/ansible_dr_fetched_backups" # Directory on the Ansible Control Node

  tasks:
    - name: Ensure backup directory exists on primary
      ansible.builtin.file:
        path: "{{ backup_root }}"
        state: directory
        mode: "0750"

    - name: Dump and compress PostgreSQL database
      ansible.builtin.shell: >
        pg_dump -U {{ db_user }} {{ db_name }} | gzip -9 -c > {{ dump_file }}
      environment:
        PGPASSWORD: "{{ db_password }}"
      changed_when: true
      tags: [backup, database]

    - name: Create tarball of application data
      ansible.builtin.shell: >
        tar --create --gzip --file {{ data_archive }} -C {{ app_data_path }} .
      args:
        creates: "{{ data_archive }}"
      changed_when: true
      tags: [backup, files]

    - name: Verify backup artifacts exist
      ansible.builtin.stat:
        path: "{{ item }}"
      loop:
        - "{{ dump_file }}"
        - "{{ data_archive }}"
      register: art_stats

    - name: Fail if any artifact is missing
      ansible.builtin.assert:
        that: "item.stat.exists"
        # FIX: Changed 'item.stat.path' to 'item.item' to reference the original path string.
        fail_msg: "Backup artifact {{ item.item }} is missing!"
      loop: "{{ art_stats.results }}"

    - name: Register artifacts for fetch (primary paths)
      # Store the paths of the created backups on the primary server.
      ansible.builtin.set_fact:
        primary_backup_package: "{{ [dump_file, data_archive] }}"
      tags: always

    - name: Fetch backup artifacts from primary to control node
      # Pulls the backup files from the primary-server to the Ansible Control Node.
      ansible.builtin.fetch:
        src: "{{ item }}"
        dest: "{{ control_node_fetch_dir }}"
        flat: true # Ensures files are copied directly into dest, not into host-specific subdirectories.
                   # E.g., /tmp/ansible_dr_fetched_backups/myapp_2025...sql.gz
      loop: "{{ primary_backup_package }}"
      register: fetched_results # Register output to reconstruct paths on control node

    - name: Register fetched artifact paths on control node
      # This task processes the output of 'fetch' to create a list of paths
      # on the control node where the backups are now stored.
      ansible.builtin.set_fact:
        control_node_backup_package: |
          {% set fetched_paths = [] %}
          {% for result in fetched_results.results %}
            {% if result.dest is defined %}
              {% set _ = fetched_paths.append(result.dest) %}
            {% endif %}
          {% endfor %}
          {{ fetched_paths }}
      tags: always

###############################################################################
# PLAY 2 – TRANSFER  (Control Node ➜ DR host)  ➜  copy artifacts off-site
###############################################################################
- name: Stage 2 – Transfer (Control Node ➜ DR)
  hosts: dr # This play directly targets the DR host.
  become: true # Connects as ansibleuser, then uses sudo for root privileges on dr-server
  vars:
    dr_incoming: /srv/dr_incoming

  tasks:
    - name: Ensure incoming directory exists on DR host
      # This task now runs directly on the DR host.
      ansible.builtin.file:
        path: "{{ dr_incoming }}"
        state: directory
        mode: "0750"

    - name: Copy backup artifacts from control node to DR host
      # Copies the files that were fetched to the control node (from primary)
      # and pushes them to the DR host.
      # The 'src' path refers to the control node's filesystem.
      # The 'dest' path refers to the DR host's filesystem.
      ansible.builtin.copy:
        src: "{{ item }}"
        dest: "{{ dr_incoming }}/"
        mode: preserve # Preserves original file permissions
      # Access the 'control_node_backup_package' fact.
      # This fact was registered on the 'primary-server' host (or associated with it in Ansible's memory).
      # Since we only have one 'primary-server' in the 'primary' group, we can access its facts directly.
      loop: "{{ hostvars[groups['primary'][0]].control_node_backup_package }}"
      tags: [transfer]

###############################################################################
# PLAY 3 – RECOVER  (DR site)  ➜  restore when dr_action=failover
###############################################################################
- name: Stage 3 – Recover (only when dr_action=failover)
  hosts: dr
  become: true
  vars:
    dr_action: "{{ dr_action | default('standby') }}"
    incoming_dir: /srv/dr_incoming
    restore_root: /srv/dr_restore
    db_name: myapp
    db_user: postgres
    db_password: "{{ lookup('ansible.builtin.var', 'db_password', default='') }}"

  tasks:
    - name: Skip unless fail-over requested
      ansible.builtin.meta: end_play
      when: dr_action != 'failover'

    - name: Create restore directory
      ansible.builtin.file:
        path: "{{ restore_root }}"
        state: directory
        mode: "0750"

    - name: Find latest data archive
      ansible.builtin.find:
        paths: "{{ incoming_dir }}"
        patterns: "data_*.tgz"
        recurse: no
        age_stamp: mtime
      register: data_archives

    - name: Extract latest data archive
      ansible.builtin.unarchive:
        src: "{{ (data_archives.files | sort(attribute='mtime', reverse=true) | first).path }}"
        dest: "{{ restore_root }}"
        remote_src: true # Source file is on the remote (DR) host
      when: data_archives.matched > 0
      tags: [restore, files]

    - name: Find latest DB dump
      ansible.builtin.find:
        paths: "{{ incoming_dir }}"
        patterns: "{{ db_name }}_*.sql.gz"
        age_stamp: mtime
      register: db_dumps

    - name: Restore PostgreSQL dump
      ansible.builtin.shell: >
        bash -c 'gunzip -c "{{ (db_dumps.files | sort(attribute="mtime", reverse=true) | first).path }}" |
                  psql -U {{ db_user }} {{ db_name }}'
      environment:
        PGPASSWORD: "{{ db_password }}"
      when: db_dumps.matched > 0
      changed_when: true
      tags: [restore, database]

    - name: Start application service
      ansible.builtin.systemd:
        name: myapp
        state: started
        enabled: true
      tags: [restore, services]

    - name: Check application health endpoint
      ansible.builtin.uri:
        url: http://localhost/health
        status_code: 200
        retries: 5
        delay: 10
        register: health_check
        until: health_check.status == 200
      tags: [validate]
